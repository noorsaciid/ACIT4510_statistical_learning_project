{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25fb4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1. Imports\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae32a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded shapes\n",
      "X_train_encoded: (1920, 14)\n",
      "X_test_encoded : (600, 14)\n",
      "y_train        : (1920,)\n",
      "y_test         : (600,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. Load encoded datasets (train + test) for linear/baseline\n",
    "# ============================================\n",
    "processed_path = r\"C:\\Users\\sacii\\OneDrive\\Desktop\\Oslomet\\ACIT4510 Statistical learning\\ACIT4510_statistical_learning_project\\data\\processed\"\n",
    "\n",
    "X_train_encoded = np.load(f\"{processed_path}\\\\X_train_encoded.npy\")\n",
    "X_test_encoded  = np.load(f\"{processed_path}\\\\X_test_encoded.npy\")\n",
    "\n",
    "y_train = pd.read_csv(f\"{processed_path}\\\\y_train.csv\").squeeze()\n",
    "y_test  = pd.read_csv(f\"{processed_path}\\\\y_test.csv\").squeeze()\n",
    "\n",
    "print(\"Encoded shapes\")\n",
    "print(\"X_train_encoded:\", X_train_encoded.shape)\n",
    "print(\"X_test_encoded :\", X_test_encoded.shape)\n",
    "print(\"y_train        :\", y_train.shape)\n",
    "print(\"y_test         :\", y_test.shape)\n",
    "\n",
    "# Wrap in DataFrames (for consistency with sklearn linear models)\n",
    "n_features_enc = X_train_encoded.shape[1]\n",
    "enc_feature_names = [f\"f{i}\" for i in range(n_features_enc)]\n",
    "\n",
    "X_train_enc_df = pd.DataFrame(X_train_encoded, columns=enc_feature_names)\n",
    "X_test_enc_df  = pd.DataFrame(X_test_encoded,  columns=enc_feature_names)\n",
    "\n",
    "y_train_arr = y_train.to_numpy()\n",
    "y_test_arr  = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84efb983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost feature shapes\n",
      "X_train_xgb: (1920, 14)\n",
      "X_test_xgb : (600, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 3. Prepare XGBoost features (unscaled numeric + OHE categoricals)\n",
    "#    Recompute from raw train/test to keep notebook self-contained\n",
    "# ============================================\n",
    "raw_path = r\"C:\\Users\\sacii\\OneDrive\\Desktop\\Oslomet\\ACIT4510 Statistical learning\\ACIT4510_statistical_learning_project\\data\\raw\"\n",
    "\n",
    "train_df_raw = pd.read_csv(fr\"{raw_path}\\train_df.csv\")\n",
    "test_df_raw  = pd.read_csv(fr\"{raw_path}\\test_df.csv\")\n",
    "\n",
    "target_col = \"Happiness Score\"\n",
    "X_train_raw = train_df_raw.drop(columns=target_col).copy()\n",
    "y_train_raw = train_df_raw[target_col].copy()\n",
    "\n",
    "X_test_raw = test_df_raw.drop(columns=target_col).copy()\n",
    "y_test_raw = test_df_raw[target_col].copy()\n",
    "\n",
    "# Feature engineering for XGBoost\n",
    "exercise_map = {\"Low\": 0.0, \"Moderate\": 0.5, \"High\": 1.0}\n",
    "diet_map = {\n",
    "    \"Junk Food\": 0.0,\n",
    "    \"Keto\": 0.5,\n",
    "    \"Vegetarian\": 0.7,\n",
    "    \"Vegan\": 0.9,\n",
    "    \"Balanced\": 1.0,\n",
    "}\n",
    "stress_map = {\"High\": 0.0, \"Moderate\": 0.5, \"Low\": 1.0}\n",
    "\n",
    "def add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"Exercise_Num\"] = df[\"Exercise Level\"].map(exercise_map)\n",
    "    df[\"Diet_Num\"] = df[\"Diet Type\"].map(diet_map)\n",
    "    df[\"Stress_Num\"] = df[\"Stress Level\"].map(stress_map)\n",
    "\n",
    "    df[\"Sleep_exercise\"] = df[\"Sleep Hours\"] * df[\"Exercise_Num\"]\n",
    "    df[\"Healthy_index\"] = (df[\"Exercise_Num\"] + df[\"Diet_Num\"] + df[\"Stress_Num\"]) / 3.0\n",
    "    return df\n",
    "\n",
    "X_train_fe = add_engineered_features(X_train_raw)\n",
    "X_test_fe  = add_engineered_features(X_test_raw)\n",
    "\n",
    "numeric_features_xgb = [\n",
    "    \"Sleep Hours\",\n",
    "    \"Screen Time per Day (Hours)\",\n",
    "    \"Work Hours per Week\",\n",
    "    \"Social Interaction Score\",\n",
    "    \"Sleep_exercise\",\n",
    "    \"Healthy_index\",\n",
    "]\n",
    "\n",
    "categorical_features_xgb = [\"Exercise Level\", \"Diet Type\", \"Stress Level\"]\n",
    "\n",
    "ohe_only_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_features_xgb),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=False), categorical_features_xgb),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "X_train_xgb = ohe_only_transformer.fit_transform(X_train_fe)\n",
    "X_test_xgb  = ohe_only_transformer.transform(X_test_fe)\n",
    "\n",
    "X_train_xgb = X_train_xgb.astype(\"float32\")\n",
    "X_test_xgb  = X_test_xgb.astype(\"float32\")\n",
    "\n",
    "print(\"\\nXGBoost feature shapes\")\n",
    "print(\"X_train_xgb:\", X_train_xgb.shape)\n",
    "print(\"X_test_xgb :\", X_test_xgb.shape)\n",
    "\n",
    "# Sanity check: y consistency\n",
    "assert np.allclose(y_train_arr, y_train_raw.to_numpy())\n",
    "assert np.allclose(y_test_arr, y_test_raw.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a18c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4. Define models\n",
    "#    - Baseline (DummyRegressor)\n",
    "#    - Ridge (on encoded features)\n",
    "#    - XGBoost (on XGBoost feature set, best params from Optuna)\n",
    "# ============================================\n",
    "baseline_model = DummyRegressor(strategy=\"median\")\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "xgb_best_params = {\n",
    "    \"n_estimators\": 432,\n",
    "    \"max_depth\": 3,\n",
    "    \"learning_rate\": 0.011214720931319094,\n",
    "    \"subsample\": 0.9091705575686873,\n",
    "    \"colsample_bytree\": 0.7710206698824025,\n",
    "    \"min_child_weight\": 10,\n",
    "    \"gamma\": 4.637013156646108,\n",
    "    \"reg_alpha\": 0.0022450836842314863,\n",
    "    \"reg_lambda\": 5.305992936571093e-07,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "}\n",
    "xgb_model = XGBRegressor(**xgb_best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a66421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5. Fit and evaluate models\n",
    "# ============================================\n",
    "results = []\n",
    "\n",
    "# Baseline on encoded features\n",
    "baseline_model.fit(X_train_enc_df, y_train_arr)\n",
    "y_train_pred = baseline_model.predict(X_train_enc_df)\n",
    "y_test_pred  = baseline_model.predict(X_test_enc_df)\n",
    "\n",
    "results.append({\n",
    "    \"model\": \"Baseline_DummyMedian\",\n",
    "    \"mae_train\": mean_absolute_error(y_train_arr, y_train_pred),\n",
    "    \"rmse_train\": np.sqrt(mean_squared_error(y_train_arr, y_train_pred)),\n",
    "    \"r2_train\": r2_score(y_train_arr, y_train_pred),\n",
    "    \"mae_test\": mean_absolute_error(y_test_arr, y_test_pred),\n",
    "    \"rmse_test\": np.sqrt(mean_squared_error(y_test_arr, y_test_pred)),\n",
    "    \"r2_test\": r2_score(y_test_arr, y_test_pred),\n",
    "})\n",
    "\n",
    "# Ridge on encoded features\n",
    "ridge_model.fit(X_train_enc_df, y_train_arr)\n",
    "y_train_pred = ridge_model.predict(X_train_enc_df)\n",
    "y_test_pred  = ridge_model.predict(X_test_enc_df)\n",
    "\n",
    "results.append({\n",
    "    \"model\": \"Ridge_Alpha1\",\n",
    "    \"mae_train\": mean_absolute_error(y_train_arr, y_train_pred),\n",
    "    \"rmse_train\": np.sqrt(mean_squared_error(y_train_arr, y_train_pred)),\n",
    "    \"r2_train\": r2_score(y_train_arr, y_train_pred),\n",
    "    \"mae_test\": mean_absolute_error(y_test_arr, y_test_pred),\n",
    "    \"rmse_test\": np.sqrt(mean_squared_error(y_test_arr, y_test_pred)),\n",
    "    \"r2_test\": r2_score(y_test_arr, y_test_pred),\n",
    "})\n",
    "\n",
    "# XGBoost on XGBoost feature set\n",
    "xgb_model.fit(X_train_xgb, y_train_arr)\n",
    "y_train_pred = xgb_model.predict(X_train_xgb)\n",
    "y_test_pred  = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "results.append({\n",
    "    \"model\": \"XGBoost_OptunaBest\",\n",
    "    \"mae_train\": mean_absolute_error(y_train_arr, y_train_pred),\n",
    "    \"rmse_train\": np.sqrt(mean_squared_error(y_train_arr, y_train_pred)),\n",
    "    \"r2_train\": r2_score(y_train_arr, y_train_pred),\n",
    "    \"mae_test\": mean_absolute_error(y_test_arr, y_test_pred),\n",
    "    \"rmse_test\": np.sqrt(mean_squared_error(y_test_arr, y_test_pred)),\n",
    "    \"r2_test\": r2_score(y_test_arr, y_test_pred),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5d176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined model comparison (train vs test):\n",
      "                      mae_train  rmse_train  r2_train  mae_test  rmse_test  \\\n",
      "model                                                                        \n",
      "Baseline_DummyMedian   2.217656    2.560881 -0.000002  2.212333   2.553214   \n",
      "Ridge_Alpha1           2.192331    2.541054  0.015422  2.227348   2.574835   \n",
      "XGBoost_OptunaBest     2.080469    2.415781  0.110108  2.214780   2.570807   \n",
      "\n",
      "                       r2_test  \n",
      "model                           \n",
      "Baseline_DummyMedian -0.000061  \n",
      "Ridge_Alpha1         -0.017071  \n",
      "XGBoost_OptunaBest   -0.013891  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 6. Build comparison table\n",
    "# ============================================\n",
    "results_df = pd.DataFrame(results).set_index(\"model\")\n",
    "\n",
    "print(\"\\nCombined model comparison (train vs test):\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acit4510-statistical-learning-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
